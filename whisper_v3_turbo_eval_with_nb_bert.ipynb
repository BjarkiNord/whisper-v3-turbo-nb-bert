{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the NPSC dataset\n",
    "dataset = load_dataset(\"NbAiLab/NPSC\",'16K_mp3_bokmaal', split=\"train\")\n",
    "\n",
    "# Save the dataset as Arrow format to disk\n",
    "dataset.save_to_disk(\"npsc_dataset_arrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbjarkinord\u001b[0m (\u001b[33mbjarkinord-none\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bjark\\miniconda3\\envs\\whisper\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\bjark\\whisper-v3-turbo-nb-bert\\wandb\\run-20241023_194205-pevif3di</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bjarkinord-none/whisper-nb-bert-semantic-training/runs/pevif3di' target=\"_blank\">twilight-vortex-26</a></strong> to <a href='https://wandb.ai/bjarkinord-none/whisper-nb-bert-semantic-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bjarkinord-none/whisper-nb-bert-semantic-training' target=\"_blank\">https://wandb.ai/bjarkinord-none/whisper-nb-bert-semantic-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bjarkinord-none/whisper-nb-bert-semantic-training/runs/pevif3di' target=\"_blank\">https://wandb.ai/bjarkinord-none/whisper-nb-bert-semantic-training/runs/pevif3di</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjark\\AppData\\Local\\Temp\\ipykernel_11980\\4171676792.py:96: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\bjark\\AppData\\Local\\Temp\\ipykernel_11980\\4171676792.py:154: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "c:\\Users\\bjark\\miniconda3\\envs\\whisper\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:599: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "c:\\Users\\bjark\\miniconda3\\envs\\whisper\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [0/11187], Loss: 4.0318\n",
      "Epoch [1/5], Batch [10/11187], Loss: 0.9202\n",
      "Epoch [1/5], Batch [20/11187], Loss: 0.8647\n",
      "Epoch [1/5], Batch [30/11187], Loss: 0.9394\n",
      "Epoch [1/5], Batch [40/11187], Loss: 0.8588\n",
      "Epoch [1/5], Batch [50/11187], Loss: 1.0080\n",
      "Epoch [1/5], Batch [60/11187], Loss: 0.9931\n",
      "Epoch [1/5], Batch [70/11187], Loss: 1.2033\n",
      "Epoch [1/5], Batch [80/11187], Loss: 1.2285\n",
      "Epoch [1/5], Batch [90/11187], Loss: 1.0457\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>combined_loss</td><td>█▁▁▁▁▁▁▂▂▁</td></tr><tr><td>learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>semantic_loss</td><td>▅▂▁▆▂█▆▄█▇</td></tr><tr><td>token_loss</td><td>█▁▁▁▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>combined_loss</td><td>1.04566</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>semantic_loss</td><td>0.42539</td></tr><tr><td>token_loss</td><td>0.83296</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-vortex-26</strong> at: <a href='https://wandb.ai/bjarkinord-none/whisper-nb-bert-semantic-training/runs/pevif3di' target=\"_blank\">https://wandb.ai/bjarkinord-none/whisper-nb-bert-semantic-training/runs/pevif3di</a><br/> View project at: <a href='https://wandb.ai/bjarkinord-none/whisper-nb-bert-semantic-training' target=\"_blank\">https://wandb.ai/bjarkinord-none/whisper-nb-bert-semantic-training</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241023_194205-pevif3di\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_from_disk\n",
    "import wandb\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"whisper-nb-bert-semantic-training\")\n",
    "\n",
    "# bert and whisper model names\n",
    "SEMANTIC_MODEL_NAME = 'NbAiLab/nb-bert-base'\n",
    "WHISPER_MODEL_NAME = 'openai/whisper-large-v3-turbo'\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load Whisper model and processor\n",
    "whisper_processor = WhisperProcessor.from_pretrained(WHISPER_MODEL_NAME)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(WHISPER_MODEL_NAME).to(device)\n",
    "whisper_processor.feature_extractor.n_frames = 3000\n",
    "\n",
    "\n",
    "# Load Norwegian BERT model and tokenizer\n",
    "semantic_tokenizer = AutoTokenizer.from_pretrained(SEMANTIC_MODEL_NAME)\n",
    "semantic_model = AutoModel.from_pretrained(SEMANTIC_MODEL_NAME).to(device)\n",
    "semantic_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Freeze semantic model parameters\n",
    "for param in semantic_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Load dataset from Arrow format\n",
    "dataset = load_from_disk(\"npsc_dataset_arrow\")\n",
    "\n",
    "# Custom Dataset class for batch processing\n",
    "class NSTSpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, processor):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.hf_dataset[idx]\n",
    "        audio = sample[\"audio\"][\"array\"]  # Audio array\n",
    "        transcript = sample[\"text\"]  # Ground truth transcription\n",
    "        return audio, transcript\n",
    "\n",
    "# Function to collate and pad the audio samples in a batch\n",
    "def collate_fn(batch):\n",
    "    # Extract the audio arrays and transcripts from the batch\n",
    "    audios = [item[0] for item in batch]          # List of audio arrays\n",
    "    transcripts = [item[1] for item in batch]     # List of transcripts\n",
    "\n",
    "    # Get the expected sampling rate from the processor\n",
    "    sampling_rate = 16000\n",
    "\n",
    "    # Process the audio data with the sampling rate\n",
    "    inputs = whisper_processor(\n",
    "        audios,\n",
    "        sampling_rate=sampling_rate,  # Pass the sampling rate here\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    audio_inputs = inputs.input_features\n",
    "\n",
    "    return audio_inputs, transcripts\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 8  # Adjust based on available memory\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 5\n",
    "SEMANTIC_LOSS_WEIGHT = 0.5  # Weighting factor λ for semantic loss\n",
    "GRAD_CLIP = 1.0  # Gradient clipping threshold\n",
    "TOTAL_STEPS = 100  # Run for 100 steps\n",
    "\n",
    "# Prepare the dataset and dataloader\n",
    "nst_dataset = NSTSpeechDataset(dataset, whisper_processor)\n",
    "data_loader = DataLoader(nst_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(whisper_model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Define loss functions\n",
    "token_level_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define GradScaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Validation function\n",
    "def evaluate(model, data_loader, semantic_model, processor, tokenizer, device):\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for audio_inputs, ground_truth_texts in data_loader:\n",
    "            audio_inputs = audio_inputs.to(device)\n",
    "\n",
    "            # Tokenize ground truth texts\n",
    "            ground_truth_tokens = processor.tokenizer(ground_truth_texts, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "\n",
    "            # Forward pass through Whisper model\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_features=audio_inputs, labels=ground_truth_tokens)\n",
    "                logits = outputs.logits\n",
    "                token_loss = outputs.loss\n",
    "\n",
    "            # Generate predicted texts\n",
    "            predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "            predicted_texts = processor.tokenizer.batch_decode(predicted_tokens, skip_special_tokens=True)\n",
    "\n",
    "            # Compute semantic embeddings\n",
    "            gt_encoding = tokenizer(ground_truth_texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "            pred_encoding = tokenizer(predicted_texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "            gt_embeddings = semantic_model(**gt_encoding).last_hidden_state.mean(dim=1)\n",
    "            pred_embeddings = semantic_model(**pred_encoding).last_hidden_state.mean(dim=1)\n",
    "\n",
    "            # Compute cosine similarity for semantic loss\n",
    "            cosine_similarity = nn.functional.cosine_similarity(gt_embeddings, pred_embeddings, dim=-1)\n",
    "            semantic_loss = 1 - cosine_similarity.mean()\n",
    "\n",
    "            total_loss = token_loss + SEMANTIC_LOSS_WEIGHT * semantic_loss\n",
    "            total_val_loss += total_loss.item()\n",
    "\n",
    "    return total_val_loss / len(data_loader)\n",
    "\n",
    "# Training Loop\n",
    "step = 0  # Initialize step counter\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    whisper_model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, (audio_inputs, ground_truth_texts) in enumerate(data_loader):\n",
    "        audio_inputs = audio_inputs.to(device)\n",
    "        step += 1\n",
    "\n",
    "        # Tokenize ground truth texts\n",
    "        ground_truth_tokens = whisper_processor.tokenizer(\n",
    "            ground_truth_texts, return_tensors=\"pt\", padding=True\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through Whisper model\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = whisper_model(input_features=audio_inputs, labels=ground_truth_tokens)\n",
    "            logits = outputs.logits\n",
    "            token_loss = outputs.loss\n",
    "\n",
    "        # Generate predicted texts\n",
    "        predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "        predicted_texts = whisper_processor.tokenizer.batch_decode(predicted_tokens, skip_special_tokens=True) # Decode predicted tokens\n",
    "\n",
    "        # Compute semantic embeddings\n",
    "        gt_encoding = semantic_tokenizer(ground_truth_texts, return_tensors='pt', padding=True, truncation=True).to(device) # Tokenize ground truth texts\n",
    "        pred_encoding = semantic_tokenizer(predicted_texts, return_tensors='pt', padding=True, truncation=True).to(device) # Tokenize predicted texts\n",
    "\n",
    "        gt_embeddings = semantic_model(**gt_encoding).last_hidden_state.mean(dim=1) # Compute embeddings for ground truth texts\n",
    "        pred_embeddings = semantic_model(**pred_encoding).last_hidden_state.mean(dim=1) # Compute embeddings for predicted texts\n",
    "\n",
    "        # Compute cosine similarity for semantic loss and calculate loss \n",
    "        cosine_similarity = nn.functional.cosine_similarity(gt_embeddings, pred_embeddings, dim=-1)\n",
    "        semantic_loss = 1 - cosine_similarity.mean()\n",
    "\n",
    "        # Combine losses\n",
    "        total_batch_loss = token_loss + SEMANTIC_LOSS_WEIGHT * semantic_loss\n",
    "\n",
    "        # Backpropagation with mixed precision\n",
    "        scaler.scale(total_batch_loss).backward()\n",
    "\n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(whisper_model.parameters(), GRAD_CLIP)\n",
    "\n",
    "        # Update parameters\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Scheduler step (after optimizer.step())\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += total_batch_loss.item()\n",
    "\n",
    "        # Logging to WandB\n",
    "        if batch_idx % 10 == 0:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            wandb.log({\n",
    "                \"token_loss\": token_loss.item(),\n",
    "                \"semantic_loss\": semantic_loss.item(),\n",
    "                \"combined_loss\": total_batch_loss.item(),\n",
    "                \"learning_rate\": current_lr,\n",
    "                \"step\": step,\n",
    "                \"epoch\": epoch\n",
    "            })\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{batch_idx}/{len(data_loader)}], Loss: {total_batch_loss.item():.4f}\")\n",
    "        \n",
    "        # implement best model saving\n",
    "        if total_batch_loss < best_loss:\n",
    "            best_loss = total_batch_loss\n",
    "            torch.save(whisper_model.state_dict(), 'best_model.pth')\n",
    "            torch.save(whisper_processor, 'best_processor.pth')\n",
    "\n",
    "        # early stopping\n",
    "        if total_batch_loss > best_loss:\n",
    "            patience += 1\n",
    "        else:\n",
    "            patience = 0\n",
    "\n",
    "\n",
    "\n",
    "        # Check if total steps reached\n",
    "        if step >= TOTAL_STEPS:\n",
    "            break  # Exit the inner loop\n",
    "\n",
    "    if step >= TOTAL_STEPS:\n",
    "        break  # Exit the outer loop\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Validation step\n",
    "    val_loss = evaluate(whisper_model, data_loader, semantic_model, whisper_processor, semantic_tokenizer, device)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Training Loss: {average_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Log validation loss to WandB\n",
    "    wandb.log({\n",
    "        \"train_loss\": average_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "    })\n",
    "\n",
    "# Save the fine-tuned model\n",
    "whisper_model.save_pretrained('fine-tuned-whisper-model')\n",
    "whisper_processor.save_pretrained('fine-tuned-whisper-processor')\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-nbbert-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
